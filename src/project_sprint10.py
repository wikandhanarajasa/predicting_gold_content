# -*- coding: utf-8 -*-
"""Project Sprint10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EZK6gF2eDfT0aPBbTw8Xy1fXV5IuRfYk

# **Predicting Gold Content in Gold Ore Using Machine Learning**

## **Introduction**

The objective of this project is to develop a machine learning model capable of predicting the gold content within gold ore samples. By leveraging advanced data analysis techniques and machine learning algorithms, this project aims to enhance the accuracy of gold content predictions, which is critical for optimizing mining processes and resource management.

### **Goal**

The primary goal of this project is to identify the most effective machine learning model for predicting gold content in gold ore. This involves selecting the best model based on its performance and ability to accurately forecast gold content from the available data.

### **Project Stages**

### 1. Data Loading and Preparation

- **Data Acquisition**: Load the datasets.
- **Initial Analysis**: Examine the data characteristics.
- **Error Calculation**: Compute the Mean Absolute Error (MAE).
- **Data Cleaning**: Address missing columns and anomalies.

### 2. Exploratory Data Analysis (EDA)

- **Feature Analysis**: Investigate concentration changes in Au, Ag, and Pb.
- **Distribution Study**: Analyze feature distributions in training and test datasets.
- **Abnormality Detection**: Identify and handle anomalies in the data.

### 3. Model Development

- **sMAPE Calculation**: Develop a function to calculate symmetric Mean Absolute Percentage Error (sMAPE).
- **Model Training**: Train various machine learning models and evaluate them using cross-validation.
- **Model Evaluation**: Select and test the best model using the test dataset.

### **Dataset Description**

The datasets for this project are provided in three files:
- `gold_recovery_train.csv`: Training data
- `gold_recovery_test.csv`: Test data
- `gold_recovery_full.csv`: Complete dataset with all features

These files are indexed by date and time of acquisition. Some features might be missing in the test set due to timing differences in measurements. The source dataset provides a comprehensive view of all features.

## **Data Loadment**

Load the libraries that needed in this project
"""

# Libraries for data processing
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Libraries for statistic test
from scipy.stats import levene
from sklearn.metrics import mean_absolute_error

# Libraries for data visualization
import seaborn as sns
import matplotlib.pyplot as plt

# Libraries for machine learning development
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

# Libraries for metric
from sklearn.model_selection import cross_validate

# Path for training dataset
path0 = '/content/gold_recovery_train.csv'

# Path for test dataset
path1 = '/content/gold_recovery_test.csv'

#Path for whole source of dataset
path2 = '/content/gold_recovery_full.csv'

# Loading training dataset

train = pd.read_csv(path0)
train.head(2)

# Loading test dataset

test = pd.read_csv(path1)
test.head(2)

# Loading source dataset

source = pd.read_csv(path2)
source.head(2)

"""### **Data Checking**

Study general information of each data

#### **Training Dataset**
"""

# Understand the number of columns and rows

train.shape

# Understand the general information regarding the columns content

train.info()

# Check missing value

train.isnull().sum()

# Check duplicated value

train.duplicated().sum()

"""#### **Test Dataset**"""

# Understand the number of columns and rows

test.shape

# Understand the general information regarding the columns content

test.info()

# Check missing value

test.isnull().sum()

# Check duplicated value

train.duplicated().sum()

"""#### **Source Dataset**"""

# Understand the number of columns and rows

source.shape

16860 + 5856

# Understand the general information regarding the columns content

source.info()

# Check missing value

source.isnull().sum()

# Check duplicated value

source.duplicated().sum()

"""### **Mean Absolut Error (MEA) Calculation**

In this case we will check whether the value in column rougher.output.recovery is well calculated by comparing with the manual calculation of output process of below and find the mean absolut error value.

Output = (C x (F - T)) / (F x (C - T))

Where:

- C is the percentage of gold in the concentrate right after the flotation process (to determine the recovery of the rough concentrate) or after purification (to determine the recovery of the final concentrate).
- F is the percentage of gold in the feed before the flotation process (to determine the recovery of the rough concentrate) / in the concentrate right after the flotation process (to determine the recovery of the final concentrate).
- T is the percentage of gold in the rough tailings, right after the flotation process (to determine the recovery of the rough concentrate) / after purification (to determine the recovery of the final concentrate).

Thus:

- C is the value in column 'rougher.output.concentrate_au'.
- F is the value in column 'rougher.input.feed_au'.
- T is the value in column 'final.output.tail_au'.


This calculation will use the train dataset.
"""

C = train['rougher.output.concentrate_au']
F = train['rougher.input.feed_au']
T = train['final.output.tail_au']

output = (C * (F - T))/(F * (C - T))
output.replace(np.inf, 0)
output

# MAE Calculation
abs(train['rougher.output.recovery'] - output).replace([np.inf, -np.inf], np.nan).mean()

"""### **Missing column between dataset**

In order to develop machine learning model, the number of column has to be the same.
"""

diff_cols_train = [column for column in train.columns if column not in test.columns]
diff_cols_train

diff_cols_test = [column for column in test.columns if column not in train.columns]
diff_cols_test

"""**Handling missing value**

The target missing value must be dropped or make sure are values are not empty.
"""

train = train.loc[(~train['rougher.output.recovery'].isnull()) & (~train['final.output.recovery'].isnull())]

"""**Split data into features and target**

In order to develop machine learning model, the data set should be differentiate between features and target set.


"""

# Differentiate the data into features(X) and target(y).

X_general = train.drop(diff_cols_train, axis=1).drop('date', axis=1)
y_general = train[['final.output.recovery', 'rougher.output.recovery']]

X_general.head(2)

y_general.head(2)

"""### **Data Preparation**

### **Handling missing values**

Both missing value in train set and test set must be filled. In this case the missing value of test set could be fill as well with the mean value.
"""

# Filling missing value with mean value

for col in X_general.columns:
    mean = X_general[col].mean()
    X_general[col] = X_general[col].fillna(mean)
    test[col] = test[col].fillna(mean)

X_general.isnull().sum()

test.isnull().sum()

"""#### **Split the dataset into train and validation**"""

# Train and validation split

X_train, X_val, y_train, y_val = train_test_split(X_general, y_general, test_size=0.25, random_state=12345)

X_train.shape

X_val.shape

y_train.shape

y_val.shape

"""**Insight**

Based on the data given, below are the condition and treatment for each condition:

1. The number of rows each data are:

- Train dataset : 16860 rows and 87 columns
- Test dataset  : 5856 rows and 53 columns
- Source dataset: 22716 rows and 87 columns

2. Missing value are detected in all dataset.
3. No duplicated value has been found in all of dataset.
4. Mean Absolute Error (MAE) has been calculated and the value is 81.79275230758236.
5. The column differences between train and test dataset solved by dropping all columns in train dataset that not exist in test dataset. Meanwhile all columns in test dataset are detected in train dataset. Thus the columns in both dataset are the same.
6. The missing value in both dataset has been filled with mean value of each columns.
7. In order to develop machine learning  model, the train dataset has been splitted into features and target of train set and validation set with validation size of 25% and train set of 75%.

## **Exploratory Data Analysis**

### **Analysis of changement of Au, Ag, and Pb concentrate after each process of purification**

The process of purrification are listed below:

1. Flotation
2. First step of cleansing
3. Second step of cleansing

The material flows before and after each process can be named input and output. From the data given we can conclude that the material are:
"""

input = [column for column in train.columns if 'input' in column]
input

output = [column for column in train.columns if 'output' in column]
output

"""Input Flotation:

1. rougher.input.feed_au
2. rougher.input.feed_ag
3. rougher.input.feed_pb

Output Flotation:

1. rougher.output.concentrate_au
2. rougher.output.concentrate_ag
3. rougher.output.concentrate_pb

Output First step of cleansing:

1. primary_cleaner.output.concentrate_au
2. primary_cleaner.output.concentrate_ag
3. primary_cleaner.output.concentrate_pb

Output Second step of cleansing:

1. final.output.concentrate_au
2. final.output.concentrate_ag
3. final.output.concentrate_pb

#### **Au - Gold**
"""

au_process = train[['rougher.input.feed_au', 'rougher.output.concentrate_au', 'primary_cleaner.output.concentrate_au', 'final.output.concentrate_au']].reset_index(drop=True).copy()

au_process

au_flow = au_process.mean().reset_index(name='mean_value')
au_flow.columns = ['flow', 'concentrate']
au_flow

plt.title('Gold Concentrate in each step')
sns.barplot(data=au_flow, x='concentrate', y='flow')
plt.show()

"""#### **Ag - Silver**"""

ag_process = train[['rougher.input.feed_ag', 'rougher.output.concentrate_ag', 'primary_cleaner.output.concentrate_ag', 'final.output.concentrate_ag']].reset_index(drop=True).copy()

ag_process

ag_flow = ag_process.mean().reset_index(name='mean_value')
ag_flow.columns = ['flow', 'concentrate']
ag_flow

plt.title('Silver Concentrate in each step')
sns.barplot(data=ag_flow, x='concentrate', y='flow')
plt.show()

"""#### **Pb - Lead**"""

pb_process = train[['rougher.input.feed_pb', 'rougher.output.concentrate_pb', 'primary_cleaner.output.concentrate_pb', 'final.output.concentrate_pb']].reset_index(drop=True).copy()

pb_process

pb_flow = pb_process.mean().reset_index(name='mean_value')
pb_flow.columns = ['flow', 'concentrate']
pb_flow

plt.title('Lead Concentrate in each step')
sns.barplot(data=pb_flow, x='concentrate', y='flow')
plt.show()

"""### **Distribution in train and test dataset**

The hypothesis are formulated as below:

- H0 : Both train and test dataset have the same variance
- H1 : Both train and test dataset have different variance

In order to check the hypothesis, in this case levene test will be use and alpha value set in 5%.
"""

# Variance test
def levene_variance(series1, series2, alpha=0.05):
    p_value_levene = levene(series1, series2).pvalue
    print(f'P-Value: {p_value_levene}')
    print(f'Alpha  : {alpha}')
    if p_value_levene >= alpha:
        print('H0 accepted: Both train and test dataset have the same variance')
    else:
        print('H1 accepted: Both train and test dataset have different variance')

feeds = [column for column in X_general.columns if 'feed' in column]
for column in feeds:
    print(column)
    levene_variance(X_general[column], X_train[column])
    print('--------------------')

"""### **Abnormality distribution of all stages**

#### **Input Floatation**
"""

plt.figure(figsize=(10,6))
sns.violinplot(data=au_process, x='rougher.input.feed_au')
plt.show()

"""#### **Output Floatation**"""

plt.figure(figsize=(10,6))
sns.violinplot(data=au_process, x='rougher.output.concentrate_au')
plt.show()

"""#### **Output First step of purification**"""

plt.figure(figsize=(10,6))
sns.violinplot(data=au_process, x='primary_cleaner.output.concentrate_au')
plt.show()

"""#### **Output Second step of purification**"""

plt.figure(figsize=(10,6))
sns.violinplot(data=au_process, x='final.output.concentrate_au')
plt.show()

"""### **Summary**

Based on the exploratory data analysis, it summerised as follow:

1. The concentration of Au and Pb from input process, flotation, first step of purrification and second step of purrification tend to increased.
2. The concentration of Ag from input process, flotation, first step of purrification and second step of purrification tend to decreased.
3. Based on the levene test, all value tested both in train and test data set have the same variance.
4. The distribution can be written as below:

- Input flotation process shows normal distribution at 6-8 point of concentration.
- Output of flotation process shows that there is some gold ore that contain no gold, but the distribution is relatively normal with mostly have 20 point concentration after this process.
- Output of first step of purification process shows that there is some gold ore that contain no gold, but the distribution is relatively normal with mostly have 33 point concentration after this process.
- Output of second step of purification process shows that there is some gold ore that contain no gold, but the distribution is relatively normal with mostly have 46 point concentration after this process.

## **Machine Learning Model Development**

### **sMAPE Calculation**

sMAPE (symmetric mean absolute percentage error) is an accuracy calculation based on percentage (or relative) errors. The formula can be define as below:

sMAPE = (|y_actual - y_prediction|) / ((|y_actual + y_prediction|) / 2)

in order to ease the calculation, loop function will be needed.
"""

# sMAPE Calculation
def smape (y_true, y_pred):
    temp = pd.DataFrame({'true': list(y_true), 'pred': list(y_pred)})
    ape = abs(temp['true'] - temp['pred']) / ((abs(temp['true']) + abs(temp['pred'])) / 2)
    mape = ape.mean()
    return mape

def final_smape(y_trues, y_preds):
    y_trues = pd.DataFrame(y_trues)
    y_preds = pd.DataFrame(y_preds)
    rough_smape = smape(y_trues.iloc[:, 0], y_preds.iloc[:, 0])
    final_smape = smape(y_trues.iloc[:, 1], y_preds.iloc[:, 1])
    smape_overall = 0.25 * rough_smape + 0.75 * final_smape
    print(f'sMAPE value: {round(smape_overall, 2) * 100}%')
    return smape_overall

"""### **Model Development**

#### **Linear Regression**
"""

lr = LinearRegression()
lr.fit(X_train, y_train)

y_train_pred = lr.predict(X_train)
smape_train = final_smape(y_train, y_train_pred)

y_val_pred = lr.predict(X_val)
smape_train = final_smape(y_val, y_val_pred)

"""#### **Decision Tree Regressor**"""

dt = DecisionTreeRegressor(max_depth=8)
dt.fit(X_train, y_train)

y_train_pred = dt.predict(X_train)
smape_train = final_smape(y_train, y_train_pred)

y_val_pred = dt.predict(X_val)
smape_train = final_smape(y_val, y_val_pred)

"""#### **Random Forrest**"""

rf = RandomForestRegressor(n_estimators=10, max_depth=15)
rf.fit(X_train, y_train)

y_train_pred = rf.predict(X_train)
smape_train = final_smape(y_train, y_train_pred)

y_val_pred = rf.predict(X_val)
smape_train = final_smape(y_val, y_val_pred)

"""### **Cross Validation**

Since Random Forrest Model has the lowest sMAPE among other model, it will tested with cross-validation.
"""

cv_values = 10

def cv(model, X_val, y_val, cv):
    scores = cross_validate(model, X_val, y_val, cv=cv)
    final_score = sum(scores['test_score']) / len(scores['test_score'])
    print(f'Average score of evaluation for cv={cv}: {round(final_score * 100, 2)}%')
    return final_score

"""#### **Cross-validation linear regression**"""

cv(lr,X_val=X_val, y_val=y_val, cv=cv_values)

"""#### **Cross-validation linear regression**"""

cv(dt,X_val=X_val, y_val=y_val, cv=cv_values)

"""#### **Cross-validation random forest regressor**"""

cv(rf,X_val=X_val, y_val=y_val, cv=cv_values)

"""### **Summary**

There were three model of machine learning that have beed developed, those are linear regression, decision tree regressor and random forest regressor. Based on the machine learning model development, it can summarized as below.

1. The linear regression got 10% sMAPE score both for train dataset and test dataset.
2. The decision tree regressor got 7% sMAPE score for train dataset and 9% for test dataset.
3. The random forest regressor got 6% sMAPE score for train dataset and 8% for test dataset.

Cross-validation has been conducted with cv value equal to 10 and the result are below.

1. Linear regression with 25.39%
2. Decision tree regressor 16.06%
3. Random forest regressor 39.88%

## **Conclusion**

1. The goal of the project is to understand the machine learning model that best to predict the gold content within the gold ore.
2. Based on the data given, below are the condition and treatment for each condition:

- The number of rows each data are:
    - Train dataset : 16860 rows and 87 columns
    - Test dataset : 5856 rows and 53 columns
- Source dataset: 22716 rows and 87 columns
- Missing value are detected in all dataset.
- No duplicated value has been found in all of dataset.
- Mean Absolute Error (MAE) has been calculated and the value is 81.79275230758236.
- The column differences between train and test dataset solved by dropping all columns in train dataset that not exist in test dataset. Meanwhile all columns in test dataset are detected in train dataset. Thus the columns in both dataset are the same.
- The missing value in both dataset has been filled with mean value of each columns.
- In order to develop machine learning model, the train dataset has been splitted into features and target of train set and validation set with validation size of 25% and train set of 75%.

3. Based on the exploratory data analysis, it summerised as follow:

- The concentration of Au and Pb from input process, flotation, first step of purrification and second step of purrification tend to increased.
- The concentration of Ag from input process, flotation, first step of purrification and second step of purrification tend to decreased.
- Based on the levene test, all value tested both in train and test data set have the same variance.
- The distribution can be written as below:
   - Input flotation process shows normal distribution at 6-8 point of concentration.
   - Output of flotation process shows that there is some gold ore that contain no gold, but the distribution is relatively normal with mostly have 20 point concentration after this process.
   - Output of first step of purification process shows that there is some gold ore that contain no gold, but the distribution is relatively normal with mostly have 33 point concentration after this process.
   - Output of second step of purification process shows that there is some gold ore that contain no gold, but the distribution is relatively normal with mostly have 46 point concentration after this process.
   
5. There were three model of machine learning that have beed developed, those are linear regression, decision tree regressor and random forest regressor. Based on the machine learning model development, it can summarized as below.

- The linear regression got 10% sMAPE score both for train dataset and test dataset.
- The decision tree regressor got 7% sMAPE score for train dataset and 9% for test dataset.
- The random forest regressor got 6% sMAPE score for train dataset and 8% for test dataset.

6. Cross-validation has been conducted with cv value equal to 10 and the result are below.

- Linear regression with 25.39%
- Decision tree regressor 16.06%
- Random forest regressor 39.88%

7. Thus the machine learning that recommended to be run for this project is decision tree regressor due to the score that relatively low at 7% for train dataset and 9% for test set and after validated with cross-validation method it shows relatively low as well at 16.06%.
"""